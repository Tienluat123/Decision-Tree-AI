{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d856e085-3fe4-4d4e-9b0f-6937c94bceea",
   "metadata": {},
   "source": [
    "# 1. Install scikit-learn & required library\n",
    "This project require numpy, pandas, matplotlib,... and scikit-learn be installed. Run the following code to install the requirements."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tornado.escape import json_encode\n",
    "from webencodings import labels\n",
    "!python -m pip install -U scikit-learn\n",
    "!python -m pip show scikit-learn\n",
    "!python -c \"import sklearn; sklearn.show_versions()\"\n",
    "!python -m pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "339582bf455d7835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Preparing the datasets\n",
    "The following code will download the dataset heart-disease from URL.\n",
    "\n",
    "â€¢ Binary class dataset: The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is used for classifying whether a\n",
    "patient has a heart disease or not based on age, blood pressure, cholesterol level, and other\n",
    "medical indicators. This dataset includes 303 samples, with labels indicating presence (1) or\n",
    "absence (0) of heart disease. Experiments with the Cleveland database have concentrated on\n",
    "simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0)."
   ],
   "id": "143716e58c1cc261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "heart_disease_db_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "heart_disease_columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
    "\n",
    "def split_dataset(_dataset: pd.DataFrame, targets: list):\n",
    "\t\"\"\"\n",
    "\tSplit input dataset into feature and target by input targets\n",
    "\t:return: dict[\"feature\"]\n",
    "\t\"\"\"\n",
    "\texisting_columns = [col for col in targets if col in _dataset.columns]\n",
    "\tmissing_columns = [col for col in targets if col not in _dataset.columns]\n",
    "\tif missing_columns:\n",
    "\t\tprint(\"These columns are not found in the dataset:\", missing_columns)\n",
    "\treturn {\n",
    "\t\t\"feature\": _dataset.drop(existing_columns, axis=1),\n",
    "\t\t\"target\": _dataset[existing_columns],\n",
    "\t}\n",
    "\n",
    "# fetch dataset from url\n",
    "raw_heart_db = pd.read_csv(heart_disease_db_url, names=heart_disease_columns)\n",
    "raw_heart_db = raw_heart_db.replace('?', np.nan)\n",
    "raw_heart_db = raw_heart_db.dropna()\n",
    "raw_heart_db = raw_heart_db.astype(float)\n",
    "raw_heart_db['num'] = raw_heart_db['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "dataset = split_dataset(raw_heart_db, targets=['num'])  # adding columns name"
   ],
   "id": "f62eda7354ac70b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Prepare, Building and Evaluating with the decision tree\n",
    "> Required to run the code at [Section #2](#2-preparing-the-datasets) before continue!\n",
    "### 3.1 Prepare the splits for the building of the decision tree\n",
    "This following code splits the dataset into multiple splits with defined ratio.\n",
    "The splits are structured as following`[ratio: float => (feature_train, feature_test, label_train, label_test)]`"
   ],
   "id": "3f7fc10cb0bc1f30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_ratios = [0.6, 0.4, 0.2, 0.1] # train/test 60/40 40/60 80/20 90/10\n",
    "random_seed = 42                    # 42 for testing and cultural reference :)\n",
    "\n",
    "def prepare_dataset(features, labels, test_size, seed=None):\n",
    "\t\"\"\"\n",
    "\t:param test_size: Test size ratio (test/(train+test))\n",
    "\t:return: feature_train, feature_test, label_train, label_test\n",
    "\t\"\"\"\n",
    "\treturn train_test_split(features, labels, test_size=test_size, stratify=labels, random_state=seed, shuffle=True)\n",
    "\n",
    "def prepare_all_splits(features, labels, seed=None):\n",
    "    splits = {}\n",
    "    for split_ratio in split_ratios:\n",
    "        splits[split_ratio] = prepare_dataset(features, labels, test_size=split_ratio, seed=seed)\n",
    "    return splits\n",
    "\n",
    "dataset_splits = prepare_all_splits(dataset['feature'], dataset['target'], seed=random_seed)"
   ],
   "id": "f80a1f5d768132ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Training",
   "id": "6b06fc6977f1d6b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate(feature_train, label_train, max_depth=None) -> DecisionTreeClassifier:\n",
    "\tdtc = DecisionTreeClassifier(criterion='entropy', random_state=random_seed, max_depth=max_depth)\n",
    "\tdtc.fit(feature_train, label_train) # train\n",
    "\treturn dtc"
   ],
   "id": "3ddf9c4f9c636009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_all_splits(dataset_name):\n",
    "\tfor test_size in split_ratios:\n",
    "\t\t_feature_train, _feature_test, _label_train, _label_test = dataset_splits[test_size]\n",
    "\t\tdtc = train_and_evaluate(_feature_train, _label_train, max_depth=2)\n",
    "\t\tplt.figure(dpi=1200)\n",
    "\t\ttree.plot_tree(dt, rounded=True, filled=True, feature_names=feature_train.columns, class_names=[\"No Decease\", \"Decease\"])\n",
    "\t\tplt.show()\n",
    "\t\t_label_predict = dtc.predict(_feature_test) # predict the label of feature_test based on the tree\n",
    "\t\tprint(f\"\\n=== {dataset_name} Train/Test {round(test_size * 100)}/{100 - round(test_size * 100)} ===\")\n",
    "\t\tprint(classification_report(_label_test, _label_predict, target_names=[\"No Decease\", \"Decease\"]))\n",
    "\n",
    "\t\tcm = confusion_matrix(_label_test, _label_predict)\n",
    "\t\tdisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Decease\", \"Decease\"])\n",
    "\t\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\t\tdisp.plot(ax=ax, cmap='Blues')\n",
    "\t\tplt.title(f\"Confusion Matrix (Depth=2, {100 - round(test_size * 100)}/{round(test_size * 100)})\")\n",
    "\t\tplt.yticks(rotation=90)\n",
    "\t\tplt.show()\n",
    "\n",
    "run_all_split(\"Heart Disease\")"
   ],
   "id": "134b2a57420e76a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "label = [int(x > 0) for x in dataset['target']['num']]\n",
    "print(len(label))\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = prepare_dataset(dataset['feature'], label, test_size=0.2) # 80 / 20 train/test\n",
    "print(len(feature_train), len(feature_test), len(label_train), len(label_test))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2, criterion='entropy', random_state=42)\n",
    "dt.fit(feature_train, label_train)\n",
    "\n",
    "# print(dt.score(feature_test, label_test))\n",
    "\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=1200)\n",
    "# features_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]\n",
    "tree.plot_tree(dt, rounded=True, filled=True, feature_names=feature_train.columns, class_names=[\"No Decease\", \"Decease\"])\n",
    "plt.show()"
   ],
   "id": "8ba54be02bb8a297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title):\n",
    "\tprint(classification_report(y_true, y_pred, target_names=['No Decease', 'Decease']))\n",
    "\tcm = confusion_matrix(y_true, y_pred)\n",
    "\tdisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Decease\", \"Decease\"])\n",
    "\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\tdisp.plot(ax=ax, cmap='Blues_r')\n",
    "\tplt.title(title)\n",
    "\tplt.yticks(rotation=90)\n",
    "\tplt.show()\n",
    "\n",
    "y_pred = dt.predict(feature_test)\n",
    "plot_confusion(label_test, y_pred, f'Confusion Matrix')"
   ],
   "id": "4e693c9e57d0643d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.4",
   "id": "3c805628641d5624"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
