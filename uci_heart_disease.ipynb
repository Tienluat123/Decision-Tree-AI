{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d856e085-3fe4-4d4e-9b0f-6937c94bceea",
   "metadata": {},
   "source": [
    "# 1. Install scikit-learn & required library\n",
    "This project require numpy, pandas, matplotlib,... and scikit-learn be installed. Run the following code to install the requirements."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!python -m pip install -U scikit-learn\n",
    "!python -m pip show scikit-learn\n",
    "!python -c \"import sklearn; sklearn.show_versions()\"\n",
    "!python -m pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "339582bf455d7835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Preparing the datasets\n",
    "The following code will download the dataset heart-disease from URL.\n",
    "\n",
    "â€¢ Binary class dataset: The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is used for classifying whether a\n",
    "patient has a heart disease or not based on age, blood pressure, cholesterol level, and other\n",
    "medical indicators. This dataset includes 303 samples, with labels indicating presence (1) or\n",
    "absence (0) of heart disease. Experiments with the Cleveland database have concentrated on\n",
    "simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0)."
   ],
   "id": "143716e58c1cc261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "heart_disease_db_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "heart_disease_columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
    "\n",
    "def split_dataset(_dataset: pd.DataFrame, targets: list):\n",
    "\t\"\"\"\n",
    "\tSplit input dataset into feature and target by input targets\n",
    "\t:return: dict[\"feature\"]\n",
    "\t\"\"\"\n",
    "\texisting_columns = [col for col in targets if col in _dataset.columns]\n",
    "\tmissing_columns = [col for col in targets if col not in _dataset.columns]\n",
    "\tif missing_columns:\n",
    "\t\tprint(\"These columns are not found in the dataset:\", missing_columns)\n",
    "\treturn {\n",
    "\t\t\"feature\": _dataset.drop(existing_columns, axis=1),\n",
    "\t\t\"target\": _dataset[existing_columns],\n",
    "\t}\n",
    "\n",
    "# fetch dataset from url\n",
    "raw_heart_db = pd.read_csv(heart_disease_db_url, names=heart_disease_columns)\n",
    "raw_heart_db = raw_heart_db.replace('?', np.nan)\n",
    "raw_heart_db = raw_heart_db.dropna()\n",
    "raw_heart_db = raw_heart_db.astype(float)\n",
    "raw_heart_db['num'] = raw_heart_db['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "dataset = split_dataset(raw_heart_db, targets=['num'])  # adding columns name"
   ],
   "id": "f62eda7354ac70b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Prepare, Building and Evaluating with the decision tree\n",
    "> Required to run the code at [Section #2](#2-preparing-the-datasets) before continue!\n",
    "### 3.1 Prepare the splits for the building of the decision tree\n",
    "This following code splits the dataset into multiple splits with defined ratio.\n",
    "The splits are structured as following`[ratio: float => (feature_train, feature_test, label_train, label_test)]`"
   ],
   "id": "3f7fc10cb0bc1f30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_ratios = [0.6, 0.4, 0.2, 0.1] # train/test 60/40 40/60 80/20 90/10\n",
    "random_seed = 42                    # 42 for testing and cultural reference :)\n",
    "class_name = [\"No Decease\", \"Decease\"]\n",
    "\n",
    "def prepare_dataset(features, labels, test_size, seed=None):\n",
    "\t\"\"\"\n",
    "\t:param test_size: Test size ratio (test/(train+test))\n",
    "\t:return: feature_train, feature_test, label_train, label_test\n",
    "\t\"\"\"\n",
    "\treturn train_test_split(features, labels, test_size=test_size, stratify=labels, random_state=seed, shuffle=True)\n",
    "\n",
    "def prepare_all_splits(features, labels, seed=None):\n",
    "    splits = {}\n",
    "    for split_ratio in split_ratios:\n",
    "        splits[split_ratio] = prepare_dataset(features, labels, test_size=split_ratio, seed=seed)\n",
    "    return splits\n",
    "\n",
    "dataset_splits = prepare_all_splits(dataset['feature'], dataset['target'], seed=random_seed)"
   ],
   "id": "f80a1f5d768132ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Training",
   "id": "6b06fc6977f1d6b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_and_evaluate(feature_train, label_train, max_depth=None) -> DecisionTreeClassifier:\n",
    "\tdtc = DecisionTreeClassifier(criterion='entropy', random_state=random_seed, max_depth=max_depth)\n",
    "\tdtc.fit(feature_train, label_train) # train\n",
    "\treturn dtc"
   ],
   "id": "3ddf9c4f9c636009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def run_all_splits(dataset_name):\n",
    "\tfor test_size in split_ratios:\n",
    "\t\t_feature_train, _feature_test, _label_train, _label_test = dataset_splits[test_size]\n",
    "\t\tdtc = train_and_evaluate(_feature_train, _label_train, max_depth=2)\n",
    "\t\tplt.figure(dpi=1200)\n",
    "\t\tplot_tree(dtc, rounded=True, filled=True, feature_names=_feature_train.columns, class_names=class_name)\n",
    "\t\tplt.show()\n",
    "\t\t_label_predict = dtc.predict(_feature_test) # predict the label of feature_test based on the tree\n",
    "\t\tprint(f\"\\n=== {dataset_name} Train/Test {100 - round(test_size * 100)}/{round(test_size * 100)} ===\")\n",
    "\t\tprint(classification_report(_label_test, _label_predict, target_names=[\"No Decease\", \"Decease\"]))\n",
    "\n",
    "\t\tcm = confusion_matrix(_label_test, _label_predict)\n",
    "\t\tdisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_name)\n",
    "\t\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\t\tdisp.plot(ax=ax, cmap='Blues')\n",
    "\t\tplt.title(f\"Confusion Matrix (Depth=2, {100 - round(test_size * 100)}/{round(test_size * 100)})\")\n",
    "\t\tplt.yticks(rotation=90)\n",
    "\t\tplt.show()\n",
    "\n",
    "run_all_splits(\"Heart Disease\")"
   ],
   "id": "134b2a57420e76a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:19:21.241682Z",
     "start_time": "2025-04-28T10:19:20.447773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "def depth_analysis(dataset_name):\n",
    "\tdepths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\taccuracies = []\n",
    "\n",
    "\tfeature_train, feature_test, label_train, label_test = dataset_splits[0.2] # 80/20 split\n",
    "\n",
    "\tfor d in depths:\n",
    "\t\tdts = train_and_evaluate(feature_train, label_train, max_depth=d)\n",
    "\t\tresult_depth = dts.get_depth()\n",
    "\t\tlabel_predict = dts.predict(feature_test)\n",
    "\t\tacc = accuracy_score(label_test, label_predict)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\tprint(f\"Max Depth: {d}\\nActual Depth: {result_depth}\\nAccuracy: {acc:.4f}\")\n",
    "\t\t# plt.figure(dpi=1200, figsize=(max(12, result_depth * 1.5), max(6, result_depth)))\n",
    "\t\tplt.figure(dpi=1200, figsize=(12, 6))\n",
    "\t\tplot_tree(dts, filled=True, feature_names=feature_train.columns, class_names=class_name, fontsize=7)\n",
    "\t\tplt.title(f\"Decision Tree ({dataset_name} max_depth={d})\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\t# Plot accuracies\n",
    "\tplt.plot([str(d) for d in depths], accuracies, marker='o')\n",
    "\tplt.title(f\"Accuracy vs Max Depth ({dataset_name})\")\n",
    "\tplt.xlabel(\"Max Depth\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.grid()\n",
    "\tplt.show()\n",
    "depth_analysis(\"Heart Disease\")"
   ],
   "id": "4e693c9e57d0643d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m \tplt.grid()\n\u001B[32m     30\u001B[39m \tplt.show()\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[43mdepth_analysis\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHeart Disease\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mdepth_analysis\u001B[39m\u001B[34m(dataset_name)\u001B[39m\n\u001B[32m      5\u001B[39m depths = [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m, \u001B[32m6\u001B[39m, \u001B[32m7\u001B[39m]\n\u001B[32m      6\u001B[39m accuracies = []\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m feature_train, feature_test, label_train, label_test = \u001B[43mdataset_splits\u001B[49m[\u001B[32m0.2\u001B[39m] \u001B[38;5;66;03m# 80/20 split\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m depths:\n\u001B[32m     11\u001B[39m \tdts = train_and_evaluate(feature_train, label_train, max_depth=d)\n",
      "\u001B[31mNameError\u001B[39m: name 'dataset_splits' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:42:28.169842Z",
     "start_time": "2025-04-28T10:42:26.226322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!python -m pip install graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "def depth_analysis(dataset_name):\n",
    "\tdepths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\taccuracies = []\n",
    "\n",
    "\tfeature_train, feature_test, label_train, label_test = dataset_splits[0.2]\n",
    "\n",
    "\tfor d in depths:\n",
    "\t\t# Train decision tree with specified max_depth\n",
    "\t\tdts = train_and_evaluate(feature_train, label_train, max_depth=d)\n",
    "\t\tresult_depth = dts.get_depth()\n",
    "\t\tlabel_predict = dts.predict(feature_test)\n",
    "\t\tacc = accuracy_score(label_test, label_predict)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\tprint(f\"Max Depth: {d}\\nActual Depth: {result_depth}\\nAccuracy: {acc:.4f}\")\n",
    "\n",
    "\t\t# Export decision tree to DOT format\n",
    "\t\tdot_data = export_graphviz(\n",
    "\t\t\tdts,\n",
    "\t\t\tout_file=None,\n",
    "\t\t\tfeature_names=feature_train.columns,\n",
    "\t\t\tclass_names=class_name,\n",
    "\t\t\tfilled=True,\n",
    "\t\t\trounded=True,\n",
    "\t\t\tspecial_characters=True,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Render the tree using Graphviz\n",
    "\t\tgraph = graphviz.Source(dot_data, format=\"png\")\n",
    "\t\tgraph.render()\n",
    "\t\toutput_file = f\"decision_tree_{dataset_name}_depth_{d}\"\n",
    "\t\tgraph.render(output_file, cleanup=True, directory=\".\")\n",
    "\t\tprint(f\"Saved decision tree visualization: {output_file}.png\")\n",
    "\n",
    "\t# Plot accuracies\n",
    "\tplt.figure(figsize=(10, 6), dpi=150)\n",
    "\tplt.plot([str(d) for d in depths], accuracies, marker='o')\n",
    "\tplt.title(f\"Accuracy vs Max Depth ({dataset_name})\")\n",
    "\tplt.xlabel(\"Max Depth\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.grid()\n",
    "\tplt.savefig(f\"accuracy_vs_depth_{dataset_name}.png\", dpi=150)\n",
    "\tplt.close()\n",
    "\n",
    "# Run the analysis for Heart Disease dataset\n",
    "depth_analysis(\"Heart Disease\")"
   ],
   "id": "6014dd98c0225e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\nttis\\miniconda3\\lib\\site-packages (0.20.3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 51\u001B[39m\n\u001B[32m     48\u001B[39m \tplt.close()\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# Run the analysis for Heart Disease dataset\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[43mdepth_analysis\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHeart Disease\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mdepth_analysis\u001B[39m\u001B[34m(dataset_name)\u001B[39m\n\u001B[32m      7\u001B[39m depths = [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m, \u001B[32m6\u001B[39m, \u001B[32m7\u001B[39m]\n\u001B[32m      8\u001B[39m accuracies = []\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m feature_train, feature_test, label_train, label_test = \u001B[43mdataset_splits\u001B[49m[\u001B[32m0.2\u001B[39m]\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m depths:\n\u001B[32m     13\u001B[39m \t\u001B[38;5;66;03m# Train decision tree with specified max_depth\u001B[39;00m\n\u001B[32m     14\u001B[39m \tdts = train_and_evaluate(feature_train, label_train, max_depth=d)\n",
      "\u001B[31mNameError\u001B[39m: name 'dataset_splits' is not defined"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
